---
title: "Divergent transitions"
subtitle: "Bayesian regression: theory & practice"
author: "Michael Franke"
format: html
execute:
  error: false
  warning: false
  message: false
callout-appearance: simple
editor:
  markdown:
    wrap: sentence
---

# Preamble

{{< include 00-preamble.qmd >}}

# Divergent transitions, and how to tame (at least some of) them

The "eight schools" example is a classic and a simple illustration of a **hierarchical model**. There are $N =8$ pairs of observations, each pair from a different school. For each school $i$, we have an estimated effect size $y_i$ and an estimated standard error $\sigma_i$ for the reported effect size. (The experiments conducted at each school which gave us these pairs investigated whether short-term coaching has a effect on SAT scores.)

```{r}
data_eight_schools <- list(
  N = 8, 
  y = c(28,  8, -3,  7, -1,  1, 18, 12),
  sigma = c(15, 10, 16, 11,  9, 11, 10, 18)
)
```

We are interested in inferring the latent true effect size $\theta_i$ for each school $i$ that could have generated the observed effect size $y_i$ given spread $\sigma_i$.

We could assume that each school's true effect size $\theta_i$ is entirely independent of any other. In contrast, we could assume that there is a single true effect size for all schools $\theta_i = \theta_j$ for all $i$ and $j$. Or, more reasonably, we let the data decide and consider a model that tries to estimate how likely it is that $\theta_i$ and $\theta_j$ for different schools $i$ and $j$ are similar or not. 

To do so, we assume a hierarchical model. The true effect sizes $\theta_i$ and $\theta_j$ of schools $i$ and $j$ are assumed:

1. to have played a role in (stochastically) generating the observed $y_i$ and $y_j$, and
2. to be themselves (stochastically) generated by (a hierarchical) process that generates (and thereby possibly assimilates) the values of $\theta_i$ and $\theta_j$.

Concretely, the model takes the following form:

$$
\begin{align*}
y_i & \sim \mathcal{N}(\theta_i, \sigma_i) \\
\theta_i & \sim \mathcal{N}(\mu, \sigma') \\
\mu & \sim \mathcal{N}(0, 10) \\
\sigma & \sim \text{half-Cauchy}(0, 10) \\
\end{align*}
$$

## 3.a Draw the model

Draw a graphical representation of this mixture model, following the conventions outlined [here](https://michael-franke.github.io/intro-data-analysis/Chap-03-03-models-representation.html). Again, any format which we can decipher easily is fine, as long as it is practical (and fun) for you.

## 3.b Run the model, inspect and explain the divergent transitions

The Stan code for this model is shown below and also included in file `ADA-W09-Ex3a-8schools-centered.stan`. 

``` stan

data {
  int<lower=0> N;
  vector[N] y;
  vector<lower=0>[N] sigma;
}
parameters {
  real mu;
  real<lower=0> sigma_prime;
  vector[N] theta;
}
model {
  mu ~ normal(0, 10);
  sigma_prime ~ cauchy(0, 10);
  theta ~ normal(mu, sigma_prime);
  y ~ normal(theta, sigma);
}
```

```{r, results="hide", eval = T, warnings = T}
stan_fit_3a_8schoolsC <- stan(
  file = 'stan-files/8schools-centered.stan',
  data = data_eight_schools,
  seed = 1969
)
```

Normally, there are a lot of divergent transitions when you run this code:

```{r}
get_divergent_iterations(stan_fit_3a_8schoolsC) %>% sum()
```

Let's go explore these divergent transitions using `shinystan`. Execute the command below, go to the tab "Explore" in the Shiny App, select "Bivariate" and explore plots of $\sigma'$ against $\theta_i$ for different $i$. Points that experienced divergent transitions are shown in red. 

```{r, eval = F}
shinystan::launch_shinystan(stan_fit_3a_8schoolsC)
```

You can also produce your own (funnel) plots with the code shown below, which may be even clearer because it uses a log-transform. Again, points with divergencies are shown in red. 

```{r}
 bayesplot::mcmc_scatter(
  as.array(stan_fit_3a_8schoolsC),
  pars = c("theta[1]", "sigma_prime"),
  transform = list(sigma_prime = "log"),
  np = nuts_params(stan_fit_3a_8schoolsC),
  size = 1
)
```

Explain in your own intuitive terms why these divergent transitions occur. E.g., you might want to say something like: "Since the step size parameter is ..., we see divergencies ... because the more ... this variable is, the more/less ... that variable ..."

**Solution:**

Since the step size parameter for approximating the Hamiltonian dynamics is set globally, we run into divergent transitions specifically for cases where $\sigma'$ is very small (roughly $<1$), because for such smaller values of $\sigma'$, the "reasonable" values for $\theta_i$ are much more constraint / have lower variance than for higher values. That's why the globally optimal step size leads to divergences inside the narrow part of the "funnel".

## 3.c Non-centered parameterization

An alternative model, with so-called non-central parameterization does not have this problem with divergent transitions (they can still occur occasionally, though).

This non-central model can be written like so:

$$
\begin{align*}
y_i & \sim \mathcal{N}(\theta_i, \sigma_i) \\
\theta_i & = \mu + \sigma' \eta_i \\
\eta_i & \sim \mathcal{N}(0, 1) \\
\mu & \sim \mathcal{N}(0, 10) \\
\sigma & \sim \text{half-Cauchy}(0, 10) \\
\end{align*}
$$

Implement and run this model in Stan. Report if you got any divergent transitions, e.g., with command `get_divergent_iterations` applied to the `stanfit` object.

**Solution:**


```{r, results="hide", eval = T, warnings = T}
stan_fit_3c_8schoolsNC <- stan(
  file = 'stan-files/8schools-non-centered.stan',
  data = data_eight_schools
)
```

```{r}
get_divergent_iterations(stan_fit_3c_8schoolsNC) %>% sum()
```

## Explaining non-central parameterization

Let's look at a plot similar to the one we looked at for the model with central parameterization in 3.b:

```{r}
 bayesplot::mcmc_scatter(
  as.array(stan_fit_3c_8schoolsNC),
  pars = c("theta[1]", "sigma_prime"),
  transform = list(sigma_prime = "log"),
  np = nuts_params(stan_fit_3c_8schoolsNC),
  size = 1
)
```

What is the main striking difference (apart from the presence/absence of divergent transitions)? How is this difference a reason for why divergent transitions can be problematic? Is any estimated posterior mean for any parameter noticeably affected by this?

**Solution:**

The "funnel" in the non-central model fit is much "deeper". The samples for $\log \sigma'$ stopped at around 1 for the central-parameterization model. But for the non-central one they go to values of -4. While this is in log-scale, it nevertheless shows how the divergencies can cause failure to explore a reasonable chunk of the posterior space. Expectations based on samples with such restrictions can consequently be biased. Indeed, the estimated mean for $\sigma'$ is discernibly lower for the non-centralized parameterization.

