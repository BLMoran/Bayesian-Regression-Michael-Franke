---
title: "Cheat sheet: common things to do with BRMS"
subtitle: "Bayesian regression: theory & practice"
author: "Michael Franke"
format: html
editor: visual
execute:
  error: false
  warning: false
  message: false
  cache: true
callout-appearance: simple
---

This document provides a cursory run-down of common operations and manipulations for working with the `brms` package.

# Preamble

{{< include 00-preamble.qmd >}}

# Running a regression model

As a running example, we fit a multi-level model.

```{r}
#| results: hide

fit_MC <- 
  brms::brm(
    # "brmsformula" object specifies the model to fit
    formula = RT ~ condition + (1 + condition + shape | submission_id),
    # data to fit model to
    data = aida::data_MC_preprocessed |> 
      mutate(condition = factor(as.character(block), 
                                levels = c("goNoGo", "reaction", "discrimination"))),
    # "iter" is the number of iterations
    iter = 4000,
    # control parameters for MCMC
    control = list(adapt_delta = 0.9)
  )
```

## Updating a model

[fill me]{style="color:darkgreen"}

# Inspecting the model

[fill me]{style="color:darkgreen"}

-   summary stats, tidy ...
-   shinystan

## Retrieve names of model variables

```{r }
tidybayes::get_variables(fit_MC)[1:10]
```

# MCMC diagnostics

[fill me]{style="color:darkgreen"}

# Extracting samples

[fill me]{style="color:darkgreen"}

## Tidy samples with `tidybayes`

Retrieve all samples with `tidybayes::tidy_draws()`:

```{r}
tidybayes::tidy_draws(fit_MC)
```

## Getting summaries for samples

To get (Bayesian) summary statistics for a vector of samples from a parameter you can do this:

```{r}
posterior_Intercept <- 
  tidybayes::tidy_draws(fit_MC) |> 
  dplyr::pull("b_Intercept")
```

The `tidybayes::hdi` function gives the upper and lower bound of a Bayesian credible interval:

```{r}
tidybayes::hdi(posterior_Intercept, credMass = 0.90)
```

The function `aida::summarize_sample_vector` does so, too.

```{r}
aida::summarize_sample_vector(posterior_Intercept, name = "Intercept")
```

Here is how you can do this for several vectors at once:

```{r}
tidybayes::tidy_draws(fit_MC) |> 
  dplyr::select(starts_with("b_")) |> 
  pivot_longer(cols = everything()) |> 
  group_by(name) |> 
  reframe(aida::summarize_sample_vector(value)[-1])
```

# Plotting posteriors

[fill me]{style="color:darkgreen"}

-   population and group level effects

# Posterior predictives

## Visual PPCs

## Extracting samples from the posterior predictive distribution

A vanilla linear model (whether before or after being conditioned on some data), makes two kinds of predictions, namely:

1.  the central tendency of data $y$ for some predictor $x$, and
2.  the shape of the (hypothetical) data $y'$ for $x$.

Generalized linear models often also disassociate a prediction of central tendency (point 1) above, from the original linear predictor that is used to compute that prediction of central tendency. So, *all* linear models also predict:

3.  a linear predictor value given values of $x$,

but for vanilla linear models (with Gaussian likelihood functions and the identity function as a link function), there is no difference between 1 and 3.

All of these measures can be obtained from a fitted model with different functions, e.g., from the `tidyverse` package. Here, it does not matter whether the model was fitted to data or it is a "prior model", so to speak, fit with the flag `sample_prior = "only"`.

Here is an example for a logistic regression model (where all the three measures clearly show their conceptual difference).

```{r}
#| results: hide
fit_MT_logistic <- 
  brms::brm(
    formula = correct ~ group * condition,
    data    = aida::data_MT,
    family  = brms::bernoulli()
  )
```

```{r}
# 2 samples from the predicted central tendency
aida::data_MT |> 
  dplyr::select(group, condition) |> 
  unique() |> 
  tidybayes::add_epred_draws(
    fit_MT_logistic,
    ndraws = 2
    )

# 2 samples from the predictive distribution (data samples)
aida::data_MT |> 
  dplyr::select(group, condition) |> 
  unique() |> 
  tidybayes::add_predicted_draws(
    fit_MT_logistic,
    ndraws = 2
    )

# 2 samples for the linear predictor
aida::data_MT |> 
  dplyr::select(group, condition) |> 
  unique() |> 
  tidybayes::add_linpred_draws(
    fit_MT_logistic,
    ndraws = 2
    )
```

# Priors

## Inspecting default priors without running the model

```{r}
# define the model as a "brmsformula" object
myFormula <- brms::bf(RT ~ 1 + block + (1 + block | submission_id))

# get prior information
brms::get_prior(
  formula = myFormula,
  data    = aida::data_MC_preprocessed,
  family  = exgaussian()
  )

```

[explain what that means]{style="color:darkgreen"}

## Setting priors

[fill me]{style="color:darkgreen"}

## Sampling from the prior

[fill me]{style="color:darkgreen"}

## Prior predictive checks

# Under the hood: Stan code, design matrices etc.

## Extract the Stan code

```{r}
brms::stancode(fit_MC)
```

## Extract Stan data

This is the data passed to Stan. Useful for inspecting dimensions etc.

```{r}
brms::standata(fit_MC) |> names()
```

## Inspect design matrices

### Population-level effects

```{r}
X <- brms::standata(fit_MC)$X
X |> head()
```

### Group-level effects

The group-level design matrix is spread out over different variables (all names `Z_` followed by some indices), but retrievable like so:

```{r}
data4Stan <- brms::standata(fit_MC)
Z <- data4Stan[str_detect(data4Stan |> names(), "Z_")] |> as_tibble()
Z
```
