---
title: "Bayesian regression: theory & practice"
subtitle: "07: Causal inference"
author: "Michael Franke"
format: html
editor: visual
execute:
  error: false
  warning: false
  message: false
callout-appearance: simple
---

{{< include 00-preamble.qmd >}}

# Simpson's paradox: puzzle and data

Simpson's paradox is a puzzle about how the same data set can (or should be) be interpreted differently, based on different assumptions about the causal relation between variables.
Here is a fictitious data set frequently used by Judea Pearl (e.g., in Pearl, Glymour and Jewell ("Causal Inference in Statistics: A primer", 2016)), except that we here take the liberty of swapping the columns for the "blood pressure" scenario.

```{r}
##################################################
# set up the data for SP
##################################################

data_simpsons_paradox <- tibble(
  gender = c("Male", "Male", "Female", "Female"),
  bloodP = c("Low", "Low", "High", "High"),
  drug   = c("Take", "Refuse", "Take", "Refuse"),
  k      = c(81, 234, 192, 55),
  N      = c(87, 270, 263, 80),
  proportion = k/N
)

data_simpsons_paradox
```

<span style = "color:darkgreen"> TODO: describe data set & puzzle </span>  

For subsequent analysis (especially when generating predictive samples), it helps to have the data in long format.
The `uncount()` function is a great tool for this.

```{r}
# cast into long format
data_SP_long <- rbind(
  data_simpsons_paradox |> uncount(k) |> 
    mutate(recover = TRUE)  |> select(-N, -proportion),
  data_simpsons_paradox |> uncount(N-k) |> 
    mutate(recover = FALSE) |> select(-N, -proportion, -k)
)
data_SP_long
```

Let's quickly sanity-check that this did what we wanted it to:

```{r}
# sanity check if that worked
data_SP_long |> 
  group_by(gender, drug, recover) |> 
  count()
```

Here are plots of the data.
The drug increases recovery for both males and females, but decreases recovery on average.

```{r}
data_simpsons_paradox |> 
  ggplot(aes(x = drug, y = proportion, group = gender)) +
  geom_line(size = 1.2, color = project_colors[12]) + 
  geom_point(size = 3, aes(color = gender))
```


```{r}
data_simpsons_paradox |>
  group_by(drug) |> 
  summarise(proportion = sum(k) / sum(N)) |> 
  ggplot(aes(x = drug, y = proportion)) +
  geom_line(size = 1.2, aes(group = 1), color = project_colors[12]) +
  geom_point(size = 3, aes(color = drug)) 
```

# Causal analysis

<span style = "color:darkgreen"> TODO: describe what we want to do and how the purported causal structure makes a world of difference </span>  

- we want to calculate the *total causal effect (TCE)*: 

$$ P(R=1 \mid \mathit{do}(D=1)) - P(R=1 \mid \mathit{do}(D=0)) $$

# Case 1: Gender as a confounder


Since the set $\{G\}$ statisfies the backdoor criterion for the assumed causal DAG, we know that we can calculate the TCE in terms of:

$$
P(R=1 \mid \mathit{do}(D=d)) = \sum_{g \in \{0,1\}} P(R=1 \mid D=d, G=g) \ P(G=g)
$$
This means that we need to estimate two probability distributions:

1. We need to estimate $P(R=1 \mid D=d, G=g)$. This can be done with a logistic regression model, regressing $R$ on $D$ and $G$.
2. We need to estimate $P(G)$, which we can do with an intercept-only logistic regression model.

Once we have both of these models, the TCE can be calculated in a third step based on samples from the posterior predictive distribution of these models.

## Step 1: Intercept-only model for `gender`

Here is an intercept-only logistic regression model for `gender`:

```{r}
#| warning: false
#| error: false
#| results: hide

niter = 4000

fit_SP_GonIntercept <- brm(
  formula = gender ~ 1,
  data    = data_SP_long,
  family  = bernoulli(link = "logit"),
  iter    = niter
)
```

Each sample from the posterior of the `Intercept` parameter represents (a guess of) the log-odds of the `Male` category.
The posterior over the proportion of male participants can therefore be retrieved and plotted as follows (the yellow line shows the observed frequency):

```{r}
logistic <- function(x) {
  1 / (1 + exp(-x))
}

posterior_SP_GonIntercept <- tidybayes::tidy_draws(fit_SP_GonIntercept) |> 
  mutate(prop_male = logistic(b_Intercept)) |> 
  select(prop_male)

posterior_SP_GonIntercept |> 
  ggplot(aes(x = prop_male)) + 
  tidybayes::stat_halfeye() +
  geom_vline(aes(xintercept = 357/700), color = project_colors[3]) +
  xlab("proportion males") + 
  ylab("posterior density")
```


## Step 2: Regressing $R$ against $G$ and $D$

```{r}

fit_SP_RonGD <- brm(
  formula = recover ~ gender * drug,
  data    = data_SP_long,
  family  = bernoulli(link = "logit"),
  iter    = niter
)

# 3. get posterior predictive samples for model from step (2), based on
#    posterior predictive samples for model from step (1), while
#    setting D to 0 and 1

# obtain posterior predictive samples for the number of male participants:

postPred_males <- tidybayes::predicted_draws(
  object  = fit_SP_GonIntercept,
  newdata = tibble(Intercept = 1),
  value   = "gender",
  ndraws  = niter * 2
  ) |> 
  ungroup() |> 
  mutate(gender = ifelse(gender, "Male", "Female")) |> 
  select(gender)

# NB: in this case we could also have gotten this via: 
# rbinom(n=4000, p=rbeta(4000, 315+1, 700-315+1), size = 700)

# posterior predictive samples for D=1

posterior_DrugTaken <- tidybayes::epred_draws(
  object  = fit_SP_RonGD,
  newdata = postPred_males |> mutate(drug = "Take"),
  value   = "taken",
  ndraws  = niter * 2
) |> ungroup() |> 
  select(taken)

posterior_DrugRefused <- tidybayes::epred_draws(
  object  = fit_SP_RonGD,
  newdata = postPred_males |> mutate(drug = "Refuse"),
  value   = "refused",
  ndraws  = niter * 2
) |> ungroup() |> 
  select(refused)

CE_post <- cbind(posterior_DrugTaken, posterior_DrugRefused) |> 
  mutate(causal_effect = taken - refused) 

rbind(
  aida::summarize_sample_vector(CE_post$taken, "drug_taken"),
  aida::summarize_sample_vector(CE_post$refused, "drug_refused"),
  aida::summarize_sample_vector(CE_post$causal_effect, "causal_effect")
)

CE_post |> 
  ggplot(aes(x = causal_effect)) +
  tidybayes::stat_halfeye()
```


# Case 2: Blood pressure as a mediator

```{r}

###################################################
# SP2: blood pressure as a mediator
###################################################

# we just need to estimate P(R \mid D, B), so a single 
# regression model will do

fit_SP_RonBD <- brms::brm(
  formula = recover ~ drug,
  data    = data_SP_long,
  family  = bernoulli(link = "logit"),
  iter    = niter
)

posterior_DrugTaken <- 
  faintr::extract_cell_draws(fit_SP_RonBD, drug == "Take") |> 
  pull(draws) |> 
  logistic()

posterior_DrugRefused <- 
  faintr::extract_cell_draws(fit_SP_RonBD, drug == "Refuse") |> 
  pull(draws) |> 
  logistic()

posterior_causalEffect <- 
  posterior_DrugTaken - posterior_DrugRefused

rbind(
  aida::summarize_sample_vector(posterior_DrugTaken, "drug_taken"),
  aida::summarize_sample_vector(posterior_DrugRefused, "drug_refused"),
  aida::summarize_sample_vector(posterior_causalEffect, "causal_effect")
  
)

# This is the total causal effect! 
# We will not go into computing "direct causal effects" in the 
#   presence of mediators.




```

