---
title: "Bayesian regression: theory & practice"
subtitle: "07: Causal inference"
author: "Michael Franke"
format: html
editor: visual
execute:
  error: false
  warning: false
  message: false
callout-appearance: simple
---

{{< include 00-preamble.qmd >}}

# Simpson's paradox: puzzle and data

[Simpson's paradox](https://plato.stanford.edu/entries/paradox-simpson/) is a multi-faceted puzzle about how to analyse data, when the same data set can be interpreted differently, based on different assumptions about the causal relation between variables.
The following uses a slight variation of a fictitious data set frequently used by Judea Pearl (e.g., in Pearl, Glymour and Jewell ("Causal Inference in Statistics: A primer", 2016)).

There are two scenarios.

## Case 1: Gender

In the first scenario (referred to as "Case 1: Gender as a confound"), the data was collected by the following procedure:

- 700 participants were recruited, out of which 350 identify as `male` and 350 identify as `350` female
- each participant decides whether or not to take a new drug
- we observe whether the patient recovered or not

Here is the data for the first scenario:

```{r}
##################################################
# set up the data for SP
##################################################

data_simpsons_paradox <- tibble(
  gender = c("Male", "Male", "Female", "Female"),
  bloodP = c("Low", "Low", "High", "High"),
  drug   = c("Take", "Refuse", "Take", "Refuse"),
  k      = c(81, 234, 192, 55),
  N      = c(87, 270, 263, 80),
  proportion = k/N
)

data_simpsons_paradox |> select(-bloodP)
```

Research Team 1 bends over this data set and notices that the drug increases recovery for both males and females, as shown in the plot below.
Based on this, Research Team 1 concludes that the drug is effective and they recommend its usage.

```{r}
data_simpsons_paradox |> 
  ggplot(aes(x = drug, y = proportion, group = gender)) +
  geom_line(size = 1.2, color = project_colors[12]) + 
  geom_point(size = 3, aes(color = gender))
```

## Case 2: Blood pressure

But now consider a second scenario. 
Data was collected by the following process:

- 700 participants were recruited
- each participant decides whether or not to take a new drug
- we measure the blood pressure of each patient and divide by median into a `high` and `low` blood pressure group (if a patient took the drug, this measurement happens /after/ having taken the drug)
- we observe whether the patient recovered or not 

The data for this scenario look as follows:

```{r }
data_simpsons_paradox |> select(-gender)
```

(Yes, you are right! The numbers are exactly the same as before!)

Research Team 2 bends over this data set and notices that the drug decreases recovery rate in the whole population, as shown in the plot below.
Based on this, Research Team 2 concludes that the drug is /not/ effective.
They do /not/ recommend it for usage.

```{r}
data_simpsons_paradox |>
  group_by(drug) |> 
  summarise(proportion = sum(k) / sum(N)) |> 
  ggplot(aes(x = drug, y = proportion)) +
  geom_line(size = 1.2, aes(group = 1), color = project_colors[12]) +
  geom_point(size = 3, aes(color = drug)) 
```

## What's the paradox?

The puzzle here is that two research teams have reached opposite conclusions based on data which is at least numerically the exact same. 
Each team seems, at first glance, to have drawn reasonable conclusions.
How is it possible to reach opposite conclusions about whether or not to use a drug, based on the same set of numbers?

## Resolving the puzzle by causal analysis

You say: "The data sets are not the same! The numbers are, but in one case we observed `gender` in the other we observed `blood pressure`. That makes a difference, doesn't it?"

I say: "Well, okay, but not necessarily. Just having different label for levels of a categorical variable doesn't make for a different data set, does it?"

But you immediately shoot back: "Granted, but you also told us about a difference in the data-generating process. There is at least a temporal difference: `blood pressure` is measure after the treatment, but the level of `gender` was fixed already before the treatment."

"Okay" I say. "So, do you suggest a temporal analysis?"

You roll your eyes and after a few more (ridiculous) turns of this conversation, we both agree that, at least, conceptually speaking, there is a difference in the plausible *causal structure* of the involved variables.

Consider the first case.
There are three binary variables involved.
Given the temporal sequence of events in the data generating process, the likely causal relation between the variables is that `gender` may influence both `drug` (the decision to take the drug or not) and `recovery`.
Moreover, the `drug` may have influence `recovery` directly.
(The latter is, of course, what we want to find out: does the `drug` influence recovery /itself/.)

<span style = "color:darkgreen"> TODO: insert pictures of causal graph </span>

Now consider the second scenario.
Again, we have three binary variables.
But since `blood pressure` is measures /after/ the treatment, it is not plausible that it could have influence the decision of whether to take the drug or not.
Reversely, it /is/ plausible to assume, i.e., at least allow for the possibility, that `blood pressure` was affected by gender and that it may have affected `recovery`.
As before, we also make room for the possibility that `drug` may affect `recovery` also directly.

<span style = "color:darkgreen"> TODO: insert pictures of causal graph </span>

# Causal analysis

<span style = "color:darkgreen"> TODO: describe what we want to do and how the purported causal structure makes a world of difference </span>  

- we want to calculate the *total causal effect (TCE)*: 

$$ P(R=1 \mid \mathit{do}(D=1)) - P(R=1 \mid \mathit{do}(D=0)) $$

## Some data wrangling 

For subsequent analysis (especially when generating predictive samples), it helps to have the data in long format.
The `uncount()` function is a great tool for this.

```{r}
# cast into long format
data_SP_long <- rbind(
  data_simpsons_paradox |> uncount(k) |> 
    mutate(recover = TRUE)  |> select(-N, -proportion),
  data_simpsons_paradox |> uncount(N-k) |> 
    mutate(recover = FALSE) |> select(-N, -proportion, -k)
)
data_SP_long
```

Let's quickly sanity-check that this did what we wanted it to:

```{r}
# sanity check if that worked
data_SP_long |> 
  group_by(gender, drug, recover) |> 
  count()
```


# Case 1: Gender as a confounder

Since the set $\{G\}$ statisfies the backdoor criterion for the assumed causal DAG, we know that we can calculate the TCE in terms of:

$$
P(R=1 \mid \mathit{do}(D=d)) = \sum_{g \in \{0,1\}} P(R=1 \mid D=d, G=g) \ P(G=g)
$$
This means that we need to estimate two probability distributions:

1. We need to estimate $P(R=1 \mid D=d, G=g)$. This can be done with a logistic regression model, regressing $R$ on $D$ and $G$.
2. We need to estimate $P(G)$, which we can do with an intercept-only logistic regression model.

Once we have both of these models, the TCE can be calculated in a third step based on samples from the posterior predictive distribution of these models.

## Step 1: Intercept-only model for `gender`

Here is an intercept-only logistic regression model for `gender`:

```{r}
#| warning: false
#| error: false
#| results: hide

niter = 4000

fit_SP_GonIntercept <- brm(
  formula = gender ~ 1,
  data    = data_SP_long,
  family  = bernoulli(link = "logit"),
  iter    = niter
)
```

Each sample from the posterior of the `Intercept` parameter represents (a guess of) the log-odds of the `Male` category.
The posterior over the proportion of male participants can therefore be retrieved and plotted as follows (the yellow line shows the observed frequency):

```{r}
logistic <- function(x) {
  1 / (1 + exp(-x))
}

posterior_SP_GonIntercept <- tidybayes::tidy_draws(fit_SP_GonIntercept) |> 
  mutate(prop_male = logistic(b_Intercept)) |> 
  select(prop_male)

posterior_SP_GonIntercept |> 
  ggplot(aes(x = prop_male)) + 
  tidybayes::stat_halfeye() +
  geom_vline(aes(xintercept = 357/700), color = project_colors[3]) +
  xlab("proportion males") + 
  ylab("posterior density")
```


## Step 2: Regressing $R$ against $G$ and $D$

```{r}

fit_SP_RonGD <- brm(
  formula = recover ~ gender * drug,
  data    = data_SP_long,
  family  = bernoulli(link = "logit"),
  iter    = niter
)

# 3. get posterior predictive samples for model from step (2), based on
#    posterior predictive samples for model from step (1), while
#    setting D to 0 and 1

# obtain posterior predictive samples for the number of male participants:

postPred_males <- tidybayes::predicted_draws(
  object  = fit_SP_GonIntercept,
  newdata = tibble(Intercept = 1),
  value   = "gender",
  ndraws  = niter * 2
  ) |> 
  ungroup() |> 
  mutate(gender = ifelse(gender, "Male", "Female")) |> 
  select(gender)

# NB: in this case we could also have gotten this via: 
# rbinom(n=4000, p=rbeta(4000, 315+1, 700-315+1), size = 700)

# posterior predictive samples for D=1

posterior_DrugTaken <- tidybayes::epred_draws(
  object  = fit_SP_RonGD,
  newdata = postPred_males |> mutate(drug = "Take"),
  value   = "taken",
  ndraws  = niter * 2
) |> ungroup() |> 
  select(taken)

posterior_DrugRefused <- tidybayes::epred_draws(
  object  = fit_SP_RonGD,
  newdata = postPred_males |> mutate(drug = "Refuse"),
  value   = "refused",
  ndraws  = niter * 2
) |> ungroup() |> 
  select(refused)

CE_post <- cbind(posterior_DrugTaken, posterior_DrugRefused) |> 
  mutate(causal_effect = taken - refused) 

rbind(
  aida::summarize_sample_vector(CE_post$taken, "drug_taken"),
  aida::summarize_sample_vector(CE_post$refused, "drug_refused"),
  aida::summarize_sample_vector(CE_post$causal_effect, "causal_effect")
)

CE_post |> 
  ggplot(aes(x = causal_effect)) +
  tidybayes::stat_halfeye()
```


# Case 2: Blood pressure as a mediator

```{r}

###################################################
# SP2: blood pressure as a mediator
###################################################

# we just need to estimate P(R \mid D, B), so a single 
# regression model will do

fit_SP_RonBD <- brms::brm(
  formula = recover ~ drug,
  data    = data_SP_long,
  family  = bernoulli(link = "logit"),
  iter    = niter
)

posterior_DrugTaken <- 
  faintr::extract_cell_draws(fit_SP_RonBD, drug == "Take") |> 
  pull(draws) |> 
  logistic()

posterior_DrugRefused <- 
  faintr::extract_cell_draws(fit_SP_RonBD, drug == "Refuse") |> 
  pull(draws) |> 
  logistic()

posterior_causalEffect <- 
  posterior_DrugTaken - posterior_DrugRefused

rbind(
  aida::summarize_sample_vector(posterior_DrugTaken, "drug_taken"),
  aida::summarize_sample_vector(posterior_DrugRefused, "drug_refused"),
  aida::summarize_sample_vector(posterior_causalEffect, "causal_effect")
  
)

# This is the total causal effect! 
# We will not go into computing "direct causal effects" in the 
#   presence of mediators.




```

